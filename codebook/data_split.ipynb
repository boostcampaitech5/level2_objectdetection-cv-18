{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, train_test_split\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "annotation = \"/opt/ml/dataset/train_origin.json\"\n",
    "output_path = \"/opt/ml/dataset\"  # json 파일을 저장할 경로\n",
    "\n",
    "# json 파일 열어서 데이터를 불러온다.\n",
    "with open(annotation) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 이미지 id와 카테고리 id(클래스)를 리스트에 저장한다.\n",
    "var = [(ann[\"image_id\"], ann[\"category_id\"]) for ann in data[\"annotations\"]]\n",
    "\n",
    "X = np.ones((len(data[\"annotations\"]), 1))  # 모든 데이터 포인트에 대해 1로 이루어진 X 배열을 만든다.\n",
    "y = np.array([v[1] for v in var])  # 각 데이터 포인트의 카테고리 id(클래스)를 y 배열에 저장한다.\n",
    "groups = np.array([v[0] for v in var])  # 각 데이터 포인트의 이미지 id를 groups 배열에 저장한다.\n",
    "\n",
    "# StratifiedGroupKFold로 데이터를 5개로 나눠 교차 검증을 진행한다.\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=18)\n",
    "\n",
    "# 각 split에서 train, validation 데이터의 이미지 id와 카테고리 id를 출력한다.\n",
    "# for train_idx, valid_idx in cv.split(X, y, groups):\n",
    "#     print(\"TRAIN:\", groups[train_idx])\n",
    "#     print(\"      \", y[train_idx])\n",
    "#     print(\" TEST:\", groups[valid_idx])\n",
    "#     print(\"      \", y[valid_idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드는 다음과 같이 작동합니다:\n",
    "\n",
    "1. train.json 파일을 엽니다.\n",
    "2. annotations에서 각 이미지와 해당 이미지에 대한 주석(annotation)을 가져와서 이미지 ID와 카테고리 ID를 저장합니다.\n",
    "3. 모든 데이터 포인트에 대해 1로 이루어진 배열 X를 생성합니다.\n",
    "4. 각 데이터 포인트의 카테고리 ID를 y에 저장합니다.\n",
    "5. 각 데이터 포인트의 이미지 ID를 groups에 저장합니다.\n",
    "6. StratifiedGroupKFold를 사용하여 데이터를 5개로 나눠 교차 검증을 진행합니다.\n",
    "7. 각 split에서 train, validation 데이터의 이미지 ID와 카테고리 ID를 출력합니다.\n",
    "\n",
    "즉, 이 코드는 이미지 분류 모델을 학습시키기 위한 데이터를 준비하는 단계 중 하나인 교차 검증을 수행합니다. \n",
    "\n",
    "교차 검증을 통해 모델의 일반화 성능을 평가할 수 있습니다. StratifiedGroupKFold를 사용하여 데이터를 그룹화하고, 각 fold에서 각 그룹의 비율을 고려하여 데이터를 분할합니다. \n",
    "\n",
    "이를 통해 모델이 특정 그룹에 대해 overfitting되는 것을 방지할 수 있습니다. 출력된 결과를 통해 각 fold에서 train 데이터와 validation 데이터의 이미지 ID와 카테고리 ID를 확인할 수 있습니다.\n",
    "\n",
    "X 배열은 y와 groups 배열과 같은 shape를 갖는 배열로, 모든 데이터 포인트에 대해 1로 이루어진 배열입니다.\n",
    "\n",
    "이 배열은 StratifiedGroupKFold 함수를 사용할 때, 첫 번째 인자로서 데이터를 전달해야 하기 때문에 필요합니다. \n",
    "\n",
    "X 배열은 이 함수에서는 사용되지 않고, 학습 데이터와 검증 데이터를 나눌 때 y와 groups 정보만 사용됩니다. 따라서 X 배열은 각 데이터 포인트에 대해 동일한 가중치를 부여하고자 하는 의미 없는 배열이라고 할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_distribution 함수: 주어진 y 값의 분포를 계산하여 반환하는 함수\n",
    "def get_distribution(y):\n",
    "    y_distr = Counter(y)  # y 값의 개수를 세어 Counter 객체로 반환\n",
    "    y_vals_sum = sum(y_distr.values())  # y 값의 총 개수 계산\n",
    "\n",
    "    # 각 y 값이 전체 중 어느 정도의 비율을 차지하는지 계산하여 문자열 리스트로 반환\n",
    "    return [f\"{y_distr[i]/y_vals_sum:.2%}\" for i in range(np.max(y) + 1)]\n",
    "\n",
    "\n",
    "# 초기화\n",
    "distrs = [get_distribution(y)]  # y의 분포 계산하여 리스트에 저장\n",
    "index = [\"training set\"]  # 인덱스로 사용할 리스트 초기화\n",
    "\n",
    "# 교차 검증(fold) 수행 및 분포 계산\n",
    "for fold_ind, (train_idx, valid_idx) in enumerate(cv.split(X, y, groups)):\n",
    "    train_y, valid_y = y[train_idx], y[valid_idx]  # 교차 검증에 사용할 train, val 데이터 분리\n",
    "    train_gr, valid_gr = groups[train_idx], groups[valid_idx]  # 각 데이터의 그룹 정보 분리\n",
    "\n",
    "    # 각 fold에서 train, validation 데이터의 y 분포 계산하여 리스트에 저장\n",
    "    distrs.append(get_distribution(train_y))\n",
    "    distrs.append(get_distribution(valid_y))\n",
    "\n",
    "    # 인덱스 리스트에 train, val fold 정보 추가\n",
    "    index.append(f\"train - fold{0}\")\n",
    "    index.append(f\"val - fold{0}\")\n",
    "\n",
    "categories = [d[\"name\"] for d in data[\"categories\"]]  # 데이터셋의 카테고리 이름 리스트\n",
    "\n",
    "# 분포 결과를 데이터프레임으로 변환하여 출력\n",
    "pd.DataFrame(distrs, index=index, columns=[categories[i] for i in range(np.max(y) + 1)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 교차 검증(cross-validation)을 통해 이미지 분류 모델을 학습하기 전, 데이터셋의 레이블 분포를 확인하는 함수입니다.\n",
    "\n",
    "get_distribution 함수는 레이블(y)의 분포를 계산하고, 각 레이블의 비율을 문자열 형태로 반환합니다.\n",
    "\n",
    "distrs 리스트에는 먼저 전체 데이터셋의 레이블 분포를 저장합니다. index 리스트에는 각 분할(fold)의 이름을 저장합니다.\n",
    "\n",
    "cv.split 함수를 이용하여 데이터를 분할하고, 각 분할에서 train과 validation 데이터의 레이블 분포를 계산하여 distrs 리스트에 추가합니다. index 리스트에는 각 fold의 이름을 추가합니다.\n",
    "\n",
    "마지막으로 pd.DataFrame 함수를 이용하여 distrs 리스트를 데이터프레임으로 변환합니다. 열(column) 이름은 레이블의 이름으로, 행(row) 이름은 index 리스트의 요소로 설정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할을 위한 KFold CV를 사용하여 학습 및 검증 데이터를 나눔\n",
    "os.makedirs(os.path.join(output_path, \"k-fold\"), exist_ok=True)\n",
    "\n",
    "for fold_ind, (train_idx, valid_idx) in enumerate(cv.split(X, y, groups)):\n",
    "    # 학습 및 검증 데이터에 맞는 annotation 분리\n",
    "    train_annotations = [ann for i, ann in enumerate(data[\"annotations\"]) if i in train_idx]\n",
    "    valid_annotations = [ann for i, ann in enumerate(data[\"annotations\"]) if i in valid_idx]\n",
    "\n",
    "    # 선택된 annotation을 가진 이미지 id를 찾음\n",
    "    train_image_ids = set([ann[\"image_id\"] for ann in train_annotations])\n",
    "    valid_image_ids = set([ann[\"image_id\"] for ann in valid_annotations])\n",
    "\n",
    "    # 학습 및 검증 데이터에 맞는 이미지 분리\n",
    "    train_images = [img for img in data[\"images\"] if img[\"id\"] in train_image_ids]\n",
    "    valid_images = [img for img in data[\"images\"] if img[\"id\"] in valid_image_ids]\n",
    "\n",
    "    # 학습 및 검증 데이터에 맞는 dictionary 생성\n",
    "    train_data = {\"images\": train_images, \"annotations\": train_annotations, \"categories\": data[\"categories\"]}\n",
    "    valid_data = {\"images\": valid_images, \"annotations\": valid_annotations, \"categories\": data[\"categories\"]}\n",
    "\n",
    "    # 학습 및 검증 데이터를 각각의 json 파일로 저장\n",
    "    train_json_path = os.path.join(os.path.join(output_path, \"k-fold\"), f\"train_{fold_ind}.json\")\n",
    "    with open(train_json_path, \"w\") as f:\n",
    "        json.dump(train_data, f, indent=4)\n",
    "\n",
    "    valid_json_path = os.path.join(os.path.join(output_path, \"k-fold\"), f\"valid_{fold_ind}.json\")\n",
    "    with open(valid_json_path, \"w\") as f:\n",
    "        json.dump(valid_data, f, indent=4)\n",
    "\n",
    "# 학습 및 검증 데이터를 로드하여 데이터 유효성 검사\n",
    "for i in range(5):\n",
    "    train_json_path = os.path.join(os.path.join(output_path, \"k-fold\"), f\"train_{fold_ind}.json\")\n",
    "    valid_json_path = os.path.join(os.path.join(output_path, \"k-fold\"), f\"valid_{fold_ind}.json\")\n",
    "\n",
    "    with open(train_json_path, \"r\") as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(valid_json_path, \"r\") as f:\n",
    "        valid_data = json.load(f)\n",
    "\n",
    "    # 학습 데이터와 검증 데이터 총 이미지 개수가 4883개인지 검사\n",
    "    assert len(train_data[\"images\"]) + len(valid_data[\"images\"]) == 4883, \"데이터 유효성 검사 실패\"\n",
    "\n",
    "print(\"데이터가 모두 저장되었습니다\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드는 KFold CV를 사용하여 데이터셋을 분할하고 각각을 학습과 검증용으로 구분한 후, json 파일로 저장하는 과정을 나타낸다. 마지막으로 저장된 데이터를 다시 로드하여 데이터 유효성을 검사하는 코드까지 포함되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련셋과 테스트셋으로 분리\n",
    "train_idx, valid_idx = train_test_split(np.arange(len(data[\"images\"])), test_size=0.2, shuffle=True, random_state=18)\n",
    "print(len(train_idx), len(valid_idx))\n",
    "\n",
    "# 클래스 분포 확인을 위해 리스트 초기화\n",
    "distrs = [get_distribution(y)]\n",
    "index = [\"training set\"]\n",
    "\n",
    "# 훈련셋, 검증셋으로 분리하여 각각 클래스 분포 확인\n",
    "train_y, valid_y = y[train_idx], y[valid_idx]\n",
    "train_gr, valid_gr = groups[train_idx], groups[valid_idx]\n",
    "\n",
    "distrs.append(get_distribution(train_y))\n",
    "distrs.append(get_distribution(valid_y))\n",
    "\n",
    "index.append(f\"train - fold{0}\")\n",
    "index.append(f\"val - fold{0}\")\n",
    "\n",
    "# 카테고리명 추출\n",
    "categories = [d[\"name\"] for d in data[\"categories\"]]\n",
    "\n",
    "# 데이터프레임 생성하여 출력\n",
    "pd.DataFrame(distrs, index=index, columns=[categories[i] for i in range(np.max(y) + 1)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 데이터를 학습셋과 검증셋으로 나누고, 각 셋의 클래스 분포를 확인하기 위해 Pandas DataFrame을 사용하여 표로 출력하는 코드입니다.\n",
    "\n",
    "우선 train_test_split 함수를 사용하여 전체 데이터를 학습셋과 검증셋으로 나눕니다. 그리고 get_distribution 함수를 사용하여 클래스 분포를 계산하고, 이를 distrs 리스트에 저장합니다.\n",
    "\n",
    "그 다음, index 리스트에는 각 셋의 이름을 저장하고, categories 리스트에는 데이터셋에서 사용된 클래스의 이름을 저장합니다. 마지막으로, pd.DataFrame 함수를 사용하여 distrs 리스트를 Pandas DataFrame으로 변환하고, 셋의 이름과 클래스 이름을 이용하여 표로 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\"images\": [], \"annotations\": [], \"categories\": data[\"categories\"]}  # train set을 위한 dictionary\n",
    "valid_data = {\"images\": [], \"annotations\": [], \"categories\": data[\"categories\"]}  # valid set을 위한 dictionary\n",
    "\n",
    "# train_idx와 valid_idx를 기준으로 이미지 데이터를 train set과 test set으로 분리합니다.\n",
    "for i in train_idx:\n",
    "    image = data[\"images\"][i]\n",
    "    train_data[\"images\"].append(image)\n",
    "    for ann in data[\"annotations\"]:\n",
    "        if ann[\"image_id\"] == image[\"id\"]:\n",
    "            train_data[\"annotations\"].append(ann)\n",
    "\n",
    "for i in valid_idx:\n",
    "    image = data[\"images\"][i]\n",
    "    valid_data[\"images\"].append(image)\n",
    "    for ann in data[\"annotations\"]:\n",
    "        if ann[\"image_id\"] == image[\"id\"]:\n",
    "            valid_data[\"annotations\"].append(ann)\n",
    "\n",
    "# train set과 test set을 각각 json 파일로 저장합니다.\n",
    "train_json_path = os.path.join(output_path, \"train_split.json\")\n",
    "with open(train_json_path, \"w\") as f:\n",
    "    json.dump(train_data, f, indent=4)\n",
    "\n",
    "valid_json_path = os.path.join(output_path, \"valid_split.json\")\n",
    "with open(valid_json_path, \"w\") as f:\n",
    "    json.dump(valid_data, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_path 변수는 json 파일이 저장될 경로를 지정합니다. train_data와 test_data 변수는 각각 train set과 test set을 저장하기 위한 dictionary 입니다. \n",
    "\n",
    "train_idx와 test_idx는 데이터셋의 이미지를 분리하기 위한 인덱스이며, 이를 이용하여 이미지 데이터를 train set과 test set으로 분리합니다. 그 후, 분리된 데이터셋을 json 파일로 저장합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
